{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06bbf139-cd7b-4010-aa7c-f09885d31927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee00e51-ddd8-4668-909b-6fee28e774be",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19bacb1e-d79f-4178-810b-e1ce44d63403",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.load('predictions.npy')\n",
    "targets = np.load('targets.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a5d77b4-1af2-4938-975e-393f55c79f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 64, 256)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c2b374f-5b49-4dab-9bed-f4ec6954174c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e58ac69-b661-435f-9e36-9d6fdd84c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_targets = np.eye(predictions.shape[-1])[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71a6130-15bd-405f-9a4f-ce472a86a4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 64, 256)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d2de5c5-4030-4507-bf21-5de3d26fae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p = np.sum(reshaped_targets * predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9da289da-d8b1-4195-a2f4-039534eb2c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_pad = 1.0 - np.equal(targets, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ed928f3-af83-4621-9def-260ff96e8930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a66d681-ac74-49c4-b496-963d51a23880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e132662-bdc2-4892-88a0-84fc518d9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_log_p = log_p * non_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ca66a3f-2ccb-4020-910a-930eea1fee6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -5.39654493,  -1.03111839,  -0.66916656, ...,  -0.        ,\n",
       "         -0.        ,  -0.        ],\n",
       "       [ -4.58577061,  -1.13412857,  -8.53803253, ...,  -0.        ,\n",
       "         -0.        ,  -0.        ],\n",
       "       [ -5.22238874,  -1.28241444,  -0.17312431, ...,  -0.        ,\n",
       "         -0.        ,  -0.        ],\n",
       "       ...,\n",
       "       [ -5.39654493, -17.29168129,  -4.36076593, ...,  -0.        ,\n",
       "         -0.        ,  -0.        ],\n",
       "       [ -5.93131638, -14.24741745,  -0.26373291, ...,  -0.        ,\n",
       "         -0.        ,  -0.        ],\n",
       "       [ -5.67053604,  -0.10595131,   0.        , ...,  -0.        ,\n",
       "         -0.        ,  -0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1459f106-3e09-464b-a97f-f6d1ffca2645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_log_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6be766-a4e1-475e-8f18-41070a303414",
   "metadata": {},
   "source": [
    "## Get the probability per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6e1d734-db47-4e3d-a2c5-c41a41a8c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_sum = np.sum(real_log_p, axis=1) / np.sum(non_pad, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6121d0e-1e56-4a2d-a27e-d4d4fd777af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.46443952,  -3.46996219,  -1.72545644,  -3.05195556,\n",
       "        -2.43820807,  -1.50907578, -12.93874272,  -1.70937594,\n",
       "        -2.53130744,  -1.69242826,  -2.15547694,  -2.7938809 ,\n",
       "        -2.39598125,  -1.64991479,  -1.38344754,  -2.39099213,\n",
       "        -1.84206926,  -1.47235944,  -3.56172533,  -2.57546407,\n",
       "        -1.45711951,  -1.95714687,  -1.71366604,  -1.75605716,\n",
       "        -3.13621035,  -1.71856025,  -1.65658158,  -2.58908648,\n",
       "        -2.6263243 ,  -4.99332861,  -4.18875562,  -1.33283563])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bba7a41d-cc12-4eb1-883c-6814824562f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_perplexity = -np.mean(log_prob_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3748e52-1645-4a56-bb3c-34de48eb5659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6211854987065033"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76891fb4-5d56-40f3-9147-d5b8b93e649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = np.exp(log_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a9e9550-2558-48c0-b1b4-3b04205e35c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.752016923578548"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309eab7f-f704-4981-8e1f-82b6ed447dd3",
   "metadata": {},
   "source": [
    "### Language Model Probability and Perplexity Calculation\n",
    "\n",
    "The probability of a sequence of words (or tokens) is based on the chain rule of probability, which decomposes the joint probability into a product of conditional probabilities. This reflects the dependencies between successive words in a sequence:\n",
    "\n",
    "\n",
    "$$ P(w_1, w_2, ..., w_N) = P(w_1) \\cdot P(w_2|w_1) \\cdot ... \\cdot P(w_N|w_1, w_2, ..., w_{N-1}) $$\n",
    "Each word w_i is predicted based on the context provided by the preceding words $ w_1, ..., w_{i-1} $, capturing the sequence's inherent conditional dependencies.\n",
    "\n",
    "Taking the logarithm of this product, which is a common step in language model evaluation, gives us a sum of log probabilities:\n",
    "\n",
    "\n",
    "$$ log P(w_1, w_2, ..., w_N) = log P(w_1) + log P(w_2|w_1) + ... + log P(w_N|w_1, w_2, ..., w_{N-1}) $$\n",
    "This sum does not assume independence; rather, it includes the log of the conditional probabilities, maintaining the sequence's contextual information.\n",
    "\n",
    "Language models are trained to optimize these parameters so that the sum of the log conditional probabilities is maximized for actual sequences, reflecting the true nature of language, where each token is contextually dependent on previous tokens.\n",
    "\n",
    "#### Independence vs. Conditional Probability in Language Modeling\n",
    "Contrary to assuming independence, the sum of log probabilities in language modeling explicitly accounts for the sequence's conditional nature. The model's architecture (e.g., RNN, LSTM, Transformer) is designed to capture and utilize the contextual dependencies between tokens.\n",
    "\n",
    "In perplexity calculation:\n",
    "\n",
    "The sum of log probabilities computes the total log likelihood of the sequence under the model, considering only actual data tokens and ignoring padding.\n",
    "The average log likelihood per token is derived by normalizing this sum over the sequence's length (excluding padding).\n",
    "Perplexity is the exponential of the negative average log likelihood per token, representing the model's predictive performance, considering contextual dependencies.\n",
    "markdown\n",
    "Copy code\n",
    "This method of perplexity calculation ensures that the metric fairly reflects model performance across sequences of varying lengths and content. It allows for a robust comparison of model predictions and actual token sequences in language processing tasks.\n",
    "css\n",
    "Copy code\n",
    "\n",
    "Feel free to include this summary in your documentation or use it as a reference for explaining the concepts of probability and perplexity in the context of language models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3dcf2-42da-4333-8c38-11aa0f7b159c",
   "metadata": {},
   "source": [
    "### Independent Token Assumption in Probability\n",
    "\n",
    "When assuming that tokens in a sequence are independent, the joint probability of the sequence is the product of the probabilities of each individual token:\n",
    "\n",
    "\n",
    "$$ P(w_1, w_2, ..., w_N) = P(w_1) \\cdot P(w_2) \\cdot ... \\cdot P(w_N) $$\n",
    "\n",
    "Under this assumption, there is no dependency between tokens; the probability of a token occurring is not influenced by the preceding tokens in the sequence. Each token w_i is predicted in isolation, only based on its own probability $ P(w_i) $.\n",
    "\n",
    "This is in contrast to language models that consider conditional probabilities, where the occurrence of each word is dependent on the previous words in the sequence.\n",
    "\n",
    "\n",
    "In this representation, you've made it clear that each $w_i$ is being considered without regard to the other words, which is a simplification often not suitable for modeling language due to its inherent structure and dependencies.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e4d661-16c6-4b55-a960-5108d0541d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
