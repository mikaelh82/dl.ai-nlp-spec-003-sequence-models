{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93dc5ed1-cf81-491e-b8ae-9b4b78b61077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0186e1b2-6b60-40cd-b1c5-ae8798c6473c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_15      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_16      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_7        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">195,584</span> │ input_layer_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │ input_layer_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ sequential_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_15      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_16      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_7        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m195,584\u001b[0m │ input_layer_15[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │ input_layer_16[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ sequential_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ sequential_7[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">195,584</span> (764.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m195,584\u001b[0m (764.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">195,584</span> (764.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m195,584\u001b[0m (764.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = 500\n",
    "model_dimension = 128\n",
    "\n",
    "LSTM = Sequential()\n",
    "LSTM.add(layers.Embedding(input_dim = vocab_size, output_dim=model_dimension))\n",
    "LSTM.add(layers.LSTM(units=model_dimension, return_sequences=True))\n",
    "LSTM.add(layers.AveragePooling1D(pool_size = 2))\n",
    "LSTM.add(layers.Lambda(lambda x: math.l2_normalize(x)))\n",
    "\n",
    "input1 = layers.Input(shape=(None,))\n",
    "input2 = layers.Input(shape=(None,))\n",
    "\n",
    "conc = layers.Concatenate(axis = 1)((LSTM(input1), LSTM(input2)))\n",
    "\n",
    "Siamese = Model(inputs=(input1, input2), outputs=conc)\n",
    "\n",
    "Siamese.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc75ca-8560-462c-a05a-627cc0bb1536",
   "metadata": {},
   "source": [
    "1. Layer Dimensions\n",
    "Here's a breakdown of the layers in your LSTM model:\n",
    "\n",
    "a. Embedding Layer\n",
    "Input dimension (input_dim): The size of the vocabulary, set as 500. This is the total number of unique words (tokens) that your model can understand.\n",
    "Output dimension (output_dim): The dimensionality of the embeddings, set as 128. Each word in your vocabulary will be represented as a 128-dimensional vector.\n",
    "The output of this layer for each word in the input sequence will be a vector of size 128, leading to an output shape of (batch_size, sequence_length, 128) for a batch of sequences.\n",
    "\n",
    "b. LSTM Layer\n",
    "Units: Set to the same as model_dimension, which is 128. This parameter defines the dimensionality of the output space of the layer.\n",
    "return_sequences=True: This ensures that the LSTM outputs the hidden state at each time step, maintaining the time dimension in the output. Thus, the output shape remains (batch_size, sequence_length, 128).\n",
    "\n",
    "c. AveragePooling1D Layer\n",
    "pool_size=2: This parameter specifies the size of the pooling window. The layer will take the average of every consecutive group of 2 elements (along the time dimension of the sequence) to reduce the sequence length. This effectively downsamples the input sequence length by a factor of 2, making the output shape (batch_size, sequence_length/2, 128).\n",
    "2. Understanding pool_size=2 in AveragePooling1D\n",
    "The AveragePooling1D layer with pool_size=2 performs a downsampling operation by taking the average over a sliding window of 2 elements along the sequence's time dimension. Here's what happens:\n",
    "\n",
    "Function: It reduces the temporal resolution of the output from the previous LSTM layer. For instance, if the LSTM layer outputs a sequence of length 10, the pooling layer will output a sequence of length 5.\n",
    "\n",
    "Use Case: This is commonly used to reduce the amount of computation and the model's sensitivity to the exact positions of features in the input sequence. It can also help in smoothing out the features over time.\n",
    "Example\n",
    "\n",
    "Consider a sequence processed through these layers:\n",
    "\n",
    "Input sequence shape: (batch_size, 10, 128) from the LSTM layer.\n",
    "After AveragePooling1D(pool_size=2), the sequence shape becomes (batch_size, 5, 128).\n",
    "Additional Layers\n",
    "After the pooling layer, your code applies a lambda function to normalize the output, then processes two inputs using two LSTMs, concatenates their outputs, and builds a Siamese-style model.\n",
    "\n",
    "Summary Generation\n",
    "To see a detailed breakdown of each layer's output dimensions, you can execute Siamese.summary() in your environment with TensorFlow installed. This command prints the configuration and output shapes of each layer in your model, which can be particularly helpful for verifying layer connections and output sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb4c48-52fd-4e1a-a8f1-20939dc6702b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cda66fb-17c5-446d-9b47-bf2cfb3f9620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese model:\n",
      "\n",
      "Total layers: 4\n",
      "\n",
      "======\n",
      "layer_prefix_0: <InputLayer name=input_layer_15, built=True>\n",
      "======\n",
      "layer_prefix_1: <InputLayer name=input_layer_16, built=True>\n",
      "======\n",
      "layer_prefix_2: <Sequential name=sequential_7, built=True>\n",
      "======\n",
      "layer_prefix_3: <Concatenate name=concatenate_5, built=True>\n",
      "Detail of LSTM models:\n",
      "\n",
      "Total layers: 4\n",
      "\n",
      "======\n",
      "layer_prefix_0: <Embedding name=embedding_7, built=True>\n",
      "======\n",
      "layer_prefix_1: <LSTM name=lstm_7, built=True>\n",
      "======\n",
      "layer_prefix_2: <AveragePooling1D name=average_pooling1d_5, built=True>\n",
      "======\n",
      "layer_prefix_3: <Lambda name=lambda_5, built=True>\n"
     ]
    }
   ],
   "source": [
    "def show_layers(model, layer_prefix):\n",
    "    print(f\"Total layers: {len(model.layers)}\\n\")\n",
    "    for i in range(len(model.layers)):\n",
    "        print(\"======\")\n",
    "        print(f\"layer_prefix_{i}: {model.layers[i]}\")\n",
    "\n",
    "print('Siamese model:\\n')\n",
    "show_layers(Siamese, 'Parallel.sublayers')\n",
    "\n",
    "print('Detail of LSTM models:\\n')\n",
    "show_layers(LSTM, 'Serial.sublayers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2e714-b32d-4643-801e-0b1b755a11e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
