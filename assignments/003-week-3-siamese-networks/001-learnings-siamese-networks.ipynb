{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "435b9fe4-9a6c-4675-aaa2-f6abfafc6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "rnd.seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "846edb78-b3b1-4769-b584-80c531f69255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of questions: 404351\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('questions.csv')\n",
    "N = len(data)\n",
    "print(f'Numbers of questions: {N}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0dd0bbc4-bd5f-4105-a7ae-e2780a83d55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  qid1  qid2                                          question1  \\\n",
       "5    5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "7    7    15    16                     How can I be a good geologist?   \n",
       "11  11    23    24        How do I read and find my YouTube comments?   \n",
       "12  12    25    26               What can make Physics easy to learn?   \n",
       "13  13    27    28        What was your first sexual experience like?   \n",
       "\n",
       "                                            question2  is_duplicate  \n",
       "5   I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "7           What should I do to be a great geologist?             1  \n",
       "11             How can I see all my Youtube comments?             1  \n",
       "12            How can you make physics easy to learn?             1  \n",
       "13             What was your first sexual experience?             1  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['is_duplicate'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5871ae51-f67a-4d4d-8175-64c320d084fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 300000 Test set: 10240\n"
     ]
    }
   ],
   "source": [
    "N_train = 300000\n",
    "N_test = 10240\n",
    "data_train = data[:N_train]\n",
    "data_test = data[N_train:N_train + N_test]\n",
    "print(\"Train set:\", len(data_train), \"Test set:\", len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11176d21-1126-4361-ba1c-5e89fd2d2339",
   "metadata": {},
   "source": [
    "### Select only the question pairs that are duplicate to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dec0b68f-886e-485e-81dc-ef2c166a1fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "299995    False\n",
       "299996     True\n",
       "299997    False\n",
       "299998     True\n",
       "299999    False\n",
       "Name: is_duplicate, Length: 300000, dtype: bool"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_index = data_train['is_duplicate'] == 1\n",
    "td_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "00d467e5-55bb-4ac3-bd94-c7f7a5e22469",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_index = [i for i, is_duplicate in enumerate(td_index) if is_duplicate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3a156081-4076-4cd5-9f53-0bc3f4838e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  111486\n",
      "Indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]\n"
     ]
    }
   ],
   "source": [
    "print('Number of duplicate questions: ', len(td_index))\n",
    "print('Indexes of first ten duplicate questions:', td_index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eac411-92e1-472a-b611-ff0e7350e98d",
   "metadata": {},
   "source": [
    "### Split the data into a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8b00cfad-cdbb-4ee4-8bbb-d5b63e67c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\n",
      "is_duplicate:  1\n"
     ]
    }
   ],
   "source": [
    "print(data_train['question1'][5])\n",
    "print(data_train['question2'][5])\n",
    "print('is_duplicate: ', data_train['is_duplicate'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "56e407c7-9cd4-417c-9a0d-7fdc072ef4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_train = data_train['question1'][td_index].to_numpy()\n",
    "Q2_train = data_train['question2'][td_index].to_numpy()\n",
    "\n",
    "Q1_test = data_test['question1'].to_numpy()\n",
    "Q2_test = data_test['question2'].to_numpy()\n",
    "y_test = data_test['is_duplicate'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "38e94c5d-265c-4b93-a7f5-f976afcff301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING QUESTIONS:\n",
      "\n",
      "Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "Question 2:  I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? \n",
      "\n",
      "Question 1:  What would a Trump presidency mean for current international master’s students on an F1 visa?\n",
      "Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? \n",
      "\n",
      "TESTING QUESTIONS:\n",
      "\n",
      "Question 1:  How do I prepare for interviews for cse?\n",
      "Question 2:  What is the best way to prepare for cse? \n",
      "\n",
      "is_duplicate = 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_train[0])\n",
    "print('Question 2: ', Q2_train[0], '\\n')\n",
    "print('Question 1: ', Q1_train[5])\n",
    "print('Question 2: ', Q2_train[5], '\\n')\n",
    "\n",
    "print('TESTING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_test[0])\n",
    "print('Question 2: ', Q2_test[0], '\\n')\n",
    "print('is_duplicate =', y_test[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341c89b-ecfc-465c-be5f-4fa072cd3eac",
   "metadata": {},
   "source": [
    "### Split your training set into training/validation sets so that you can use them at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "31b71efe-210a-47bc-8c7e-7c2c9821c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  111486\n",
      "The length of the training set is:   89188\n",
      "The length of the validation set is:  22298\n"
     ]
    }
   ],
   "source": [
    "cut_off = int(len(Q1_train) * 0.8)\n",
    "train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\n",
    "val_Q1, val_Q2 = Q1_train[cut_off:], Q2_train[cut_off:]\n",
    "\n",
    "print('Number of duplicate questions: ', len(Q1_train))\n",
    "print(\"The length of the training set is:  \", len(train_Q1))\n",
    "print(\"The length of the validation set is: \", len(val_Q1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c276d3-ed5b-4ac7-a299-8ea6a915540a",
   "metadata": {},
   "source": [
    "### Learning question encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0d484402-06d8-49a9-b99d-f1b56f0f3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "text_vectorization = tf.keras.layers.TextVectorization(output_mode='int', split='whitespace', standardize='strip_punctuation')\n",
    "text_vectorization.adapt(np.concatenate([Q1_train, Q2_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "381d340e-b27e-4df7-b5f6-24f5bf6bb47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36224\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocabulary size: {text_vectorization.vocabulary_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "44de35ad-42b8-40ce-9977-33a9c706400d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first question in the train set:\n",
      "\n",
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? \n",
      "\n",
      "encoded version:\n",
      "tf.Tensor(\n",
      "[ 6984     6   178    10  8988  2442 35393   761    13  6636 28205    31\n",
      "    28   483    45    98], shape=(16,), dtype=int64) \n",
      "\n",
      "first question in the test set:\n",
      "\n",
      "How do I prepare for interviews for cse? \n",
      "\n",
      "encoded version:\n",
      "tf.Tensor([    4     8     6   160    17  2079    17 11775], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print('first question in the train set:\\n')\n",
    "print(Q1_train[0], '\\n') \n",
    "print('encoded version:')\n",
    "print(text_vectorization(Q1_train[0]),'\\n')\n",
    "\n",
    "print('first question in the test set:\\n')\n",
    "print(Q1_test[0], '\\n')\n",
    "print('encoded version:')\n",
    "print(text_vectorization(Q1_test[0]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f4154-abc0-4042-9177-f2b2c1fe5520",
   "metadata": {},
   "source": [
    "### Creating the Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0191547a-156e-40f0-9b5f-a496b3a25c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Siamese(text_vectorizer, vocab_size=36224, d_feature=128):\n",
    "    branch = tf.keras.models.Sequential(name='sequential')\n",
    "    branch.add(text_vectorizer)\n",
    "    branch.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_feature, name='embedding'))\n",
    "    branch.add(tf.keras.layers.LSTM(units = d_feature, return_sequences=True, name='LSTM'))\n",
    "    branch.add(tf.keras.layers.GlobalAveragePooling1D(name='mean'))\n",
    "    branch.add(tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x), name='out'))\n",
    "\n",
    "    input1 = tf.keras.layers.Input(shape=(1,), dtype=tf.dtypes.string, name='input_1')\n",
    "    input2 = tf.keras.layers.Input(shape=(1,), dtype=tf.dtypes.string, name='input_2')\n",
    "\n",
    "    branch1 = branch(input1)\n",
    "    branch2 = branch(input2)\n",
    "\n",
    "    conc = tf.keras.layers.Concatenate(axis=1, name='conc_1_2')([branch1, branch2])\n",
    "\n",
    "    return tf.keras.models.Model(inputs = [input1, input2], outputs=conc, name='SiameseModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6c649-50b6-4af6-ac56-e5a40487e8af",
   "metadata": {},
   "source": [
    "The Siamese model you're asking about is designed using TensorFlow and consists of several layers where two inputs go through the same series of transformations and are then combined. Let's dissect the model, focusing on the input and output dimensions of each layer, and explain the role of the LSTM layer and GlobalAveragePooling1D.\n",
    "\n",
    "**Input Layer**\n",
    "\n",
    "The model begins with two separate input layers, each accepting a single string:\n",
    "\n",
    "```python\n",
    "input1 = tf.keras.layers.Input(shape=(1,), dtype=tf.dtypes.string, name='input_1')\n",
    "input2 = tf.keras.layers.Input(shape=(1,), dtype=tf.dtypes.string, name='input_2')\n",
    "```\n",
    "\n",
    "Here, shape=(1,) indicates each input is a single string.\n",
    "\n",
    "\n",
    "**Text Vectorization**\n",
    "\n",
    "Following the inputs, the first layer within the shared branch is a text vectorizer:\n",
    "\n",
    "```python\n",
    "branch.add(text_vectorizer)\n",
    "```\n",
    "\n",
    "This layer transforms each string input into a sequence of integers, representing words or tokens from the text. The output dimension depends on the configuration of the vectorizer but typically results in a shape like (batch_size, sequence_length) where sequence_length depends on the length of the longest sequence after vectorization.\n",
    "\n",
    "\n",
    "**Embedding Layer**\n",
    "\n",
    "The next layer is an Embedding layer:\n",
    "\n",
    "```python\n",
    "branch.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_feature, name='embedding'))\n",
    "```\n",
    "\n",
    "The embedding layer converts the integer-encoded text into dense vectors of fixed size (128 in this case). It takes an input of shape (batch_size, sequence_length) and outputs an array of shape (batch_size, sequence_length, d_feature).\n",
    "\n",
    "\n",
    "**LSTM Layer**\n",
    "\n",
    "The LSTM layer follows:\n",
    "\n",
    "```python\n",
    "branch.add(tf.keras.layers.LSTM(units=d_feature, return_sequences=True, name='LSTM'))\n",
    "```\n",
    "\n",
    "This layer is configured to return sequences, meaning instead of returning just the final output of the LSTM (common in many applications like classification), it returns an output for each timestep. This is critical for the next pooling layer to compute features across all timesteps. The output shape here is (batch_size, sequence_length, d_feature).\n",
    "\n",
    "\n",
    "**GlobalAveragePooling1D**\n",
    "\n",
    "```python\n",
    "branch.add(tf.keras.layers.GlobalAveragePooling1D(name='mean'))\n",
    "```\n",
    "\n",
    "GlobalAveragePooling1D works by taking the average over the time dimension (sequence_length), effectively reducing the features of each timestep into a single vector. This operation reduces the output shape from (batch_size, sequence_length, d_feature) to (batch_size, d_feature). It helps to reduce the model's complexity by summarizing the information in the sequence, making it useful for capturing the overall sentiment or characteristics of the input sequence without overfitting to specific words.\n",
    "\n",
    "\n",
    "**Normalization Layer**\n",
    "\n",
    "A Lambda layer normalizes the data:\n",
    "\n",
    "```python\n",
    "branch.add(tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x)), name='out')\n",
    "```\n",
    "\n",
    "Normalization (here L2 normalization) ensures that the vector length is scaled to 1, often useful for improving the stability of the learning process.\n",
    "\n",
    "\n",
    "Combining Branches\n",
    "The outputs from the two input branches are then concatenated:\n",
    "\n",
    "```python\n",
    "conc = tf.keras.layers.Concatenate(axis=1, name='conc_1_2')([branch1, branch2])\n",
    "```\n",
    "\n",
    "Since each branch outputs a normalized vector of shape (batch_size, d_feature), concatenating these along axis=1 results in an output shape of (batch_size, 2*d_feature).\n",
    "\n",
    "\n",
    "**Final Model Assembly**\n",
    "\n",
    "The final model combines everything:\n",
    "\n",
    "```python\n",
    "return tf.keras.models.Model(inputs=[input1, input2], outputs=conc, name='SiameseModel')\n",
    "```\n",
    "\n",
    "This setup is typical for tasks where the relationship between two inputs needs to be learned, such as comparing the similarity between texts, which is a common use case in Siamese networks.\n",
    "\n",
    "\n",
    "**Summary**\n",
    "\n",
    "The use of return_sequences=True in the LSTM layer is critical for the GlobalAveragePooling1D layer to function correctly, as it averages over all the timestep outputs from the LSTM, providing a robust summary of the entire input sequence. This model structure is effective for learning complex patterns in sequences and comparing them in a normalized feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "af3cff4c-b92d-46dd-be26-419b707640f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"SiameseModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"SiameseModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,768,256</span> │ input_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │ input_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conc_1_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m4,768,256\u001b[0m │ input_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │ input_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conc_1_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ sequential[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,768,256</span> (18.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,768,256\u001b[0m (18.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,768,256</span> (18.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,768,256\u001b[0m (18.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_3            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,636,672</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mean (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_3            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │     \u001b[38;5;34m4,636,672\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ LSTM (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mean (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ out (\u001b[38;5;33mLambda\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,768,256</span> (18.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,768,256\u001b[0m (18.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,768,256</span> (18.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,768,256\u001b[0m (18.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Siamese(text_vectorization, vocab_size=text_vectorization.vocabulary_size())\n",
    "model.build(input_shape=(1,))\n",
    "model.summary()\n",
    "model.get_layer(name='sequential').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2f750531-ace8-4278-9ac5-d87671a82fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model,\n",
    "                          to_file='my_model.png',\n",
    "                          show_shapes=True,\n",
    "                          show_dtype=True,\n",
    "                          show_layer_names=True,\n",
    "                          rankdir=\"TB\",\n",
    "                          expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be9c4f-17f3-4aa9-a71a-647f6ce8f98d",
   "metadata": {},
   "source": [
    "### Hard negative mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5217ba5f-8363-4c46-8c41-73526566eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripletLossFn(v1, v2, margin=0.25):\n",
    "    \"\"\"Custom Loss function.\n",
    "\n",
    "    Args:\n",
    "        v1 (numpy.ndarray or Tensor): Array with dimension (batch_size, model_dimension) associated to Q1.\n",
    "        v2 (numpy.ndarray or Tensor): Array with dimension (batch_size, model_dimension) associated to Q2.\n",
    "        margin (float, optional): Desired margin. Defaults to 0.25.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss (numpy.ndarray or Tensor)\n",
    "    \"\"\"\n",
    "    scores = tf.linalg.matmul(a=v2, b=v1, transpose_b=True)\n",
    "    batch_size = tf.cast(tf.shape(v1)[0], scores.dtype)\n",
    "    \n",
    "    positive = tf.linalg.diag_part(scores)\n",
    "    negative_zero_on_duplicate = scores - tf.linalg.diag(positive)\n",
    "    mean_negative = tf.reduce_sum(negative_zero_on_duplicate, axis=1) / (batch_size - 1)\n",
    "\n",
    "    mask_exclude_positives = tf.cast((tf.eye(batch_size) == 1)|(negative_zero_on_duplicate > tf.expand_dims(positive, 1)),\n",
    "                                    scores.dtype)\n",
    "    negative_without_positive = negative_zero_on_duplicate - 2.0 * mask_exclude_positives\n",
    "    closest_negative = tf.math.reduce_max(negative_without_positive, axis=1)\n",
    "    \n",
    "    triplet_loss1 = tf.maximum(closest_negative -  positive + margin, 0)\n",
    "    triplet_loss2 = tf.maximum(mean_negative - positive + margin, 0)\n",
    "    triplet_loss = tf.reduce_sum(triplet_loss1 + triplet_loss2)\n",
    "    return triplet_loss\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4ef435d3-5f8e-4485-93eb-a01b183623ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss: tf.Tensor(0.703507682515891, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
    "v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
    "print(\"Triplet Loss:\", TripletLossFn(v1,v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "78a9b661-dd29-4773-914a-c0e982314e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripletLoss(labels, out, margin=0.25):\n",
    "    _, out_size = out.shape # get embedding size\n",
    "    v1 = out[:,:int(out_size/2)] # Extract v1 from out\n",
    "    v2 = out[:,int(out_size/2):] # Extract v2 from out\n",
    "    return TripletLossFn(v1, v2, margin=margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfef1ee-9169-4825-b057-d78d1fcb8dca",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f731b020-53a0-46cf-8c3c-4336367f06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(((train_Q1, train_Q2),tf.constant([1]*len(train_Q1))))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((val_Q1, val_Q2),tf.constant([1]*len(val_Q1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "05045c35-7e33-44d8-a0dd-e7a3d0fc761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(Siamese, TripletLoss, text_vectorizer, train_dataset, val_dataset, d_feature=128, lr=0.01, train_steps=5):\n",
    "    \"\"\"Training the Siamese Model\n",
    "\n",
    "    Args:\n",
    "        Siamese (function): Function that returns the Siamese model.\n",
    "        TripletLoss (function): Function that defines the TripletLoss loss function.\n",
    "        text_vectorizer: trained instance of `TextVecotrization` \n",
    "        train_dataset (tf.data.Dataset): Training dataset\n",
    "        val_dataset (tf.data.Dataset): Validation dataset\n",
    "        d_feature (int, optional) = size of the encoding. Defaults to 128.\n",
    "        lr (float, optional): learning rate for optimizer. Defaults to 0.01\n",
    "        train_steps (int): number of epochs\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model\n",
    "    \"\"\"\n",
    "    ## START CODE HERE ###\n",
    "\n",
    "    # Instantiate your Siamese model\n",
    "    model = Siamese(text_vectorizer,\n",
    "                    vocab_size = len(text_vectorizer.get_vocabulary()), #set vocab_size accordingly to the size of your vocabulary\n",
    "                    d_feature = d_feature)\n",
    "    # Compile the model\n",
    "    model.compile(loss=TripletLoss,\n",
    "                  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            )\n",
    "    # Train the model \n",
    "    model.fit(train_dataset,\n",
    "              epochs = train_steps,\n",
    "              validation_data = val_dataset,\n",
    "             )\n",
    "             \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "db871d82-0190-4382-a0f4-3398228cc90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 135ms/step - loss: 127.2549 - val_loss: 126.9564\n",
      "Epoch 2/6\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 172ms/step - loss: 126.8748 - val_loss: 126.3775\n",
      "Epoch 3/6\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 173ms/step - loss: 126.5148 - val_loss: 126.3211\n",
      "Epoch 4/6\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 179ms/step - loss: 126.4039 - val_loss: 126.2875\n",
      "Epoch 5/6\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 179ms/step - loss: 126.3733 - val_loss: 126.2338\n",
      "Epoch 6/6\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 179ms/step - loss: 126.3391 - val_loss: 126.1884\n"
     ]
    }
   ],
   "source": [
    "train_steps = 6\n",
    "batch_size = 256\n",
    "train_generator = train_dataset.shuffle(len(train_Q1),\n",
    "                                        seed=7, \n",
    "                                        reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
    "val_generator = val_dataset.shuffle(len(val_Q1), \n",
    "                                   seed=7,\n",
    "                                   reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
    "model = train_model(Siamese, TripletLoss,text_vectorization, \n",
    "                                            train_generator, \n",
    "                                            val_generator, \n",
    "                                            train_steps=train_steps,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469aa370-06da-4894-8259-36b0c0731331",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "22eac968-a445-4821-84fb-20b115afddd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"SiameseModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"SiameseModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,768,256</span> │ input_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │ input_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conc_1_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m4,768,256\u001b[0m │ input_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │ input_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conc_1_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ sequential[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,304,770</span> (54.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,304,770\u001b[0m (54.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,768,256</span> (18.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,768,256\u001b[0m (18.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,536,514</span> (36.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m9,536,514\u001b[0m (36.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3109299e-ace3-451d-b452-00b8a34ec12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_Q1, test_Q2, y_test, threshold, model, batch_size=64, verbose=True):\n",
    "    \"\"\"Function to test the accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "        test_Q1 (numpy.ndarray): Array of Q1 questions. Each element of the array would be a string.\n",
    "        test_Q2 (numpy.ndarray): Array of Q2 questions. Each element of the array would be a string.\n",
    "        y_test (numpy.ndarray): Array of actual target.\n",
    "        threshold (float): Desired threshold\n",
    "        model (tensorflow.Keras.Model): The Siamese model.\n",
    "        batch_size (int, optional): Size of the batches. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model\n",
    "        numpy.array: confusion matrix\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    test_gen = tf.data.Dataset.from_tensor_slices(((test_Q1, test_Q2),None)).batch(batch_size=batch_size)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    pred = model.predict(test_gen)\n",
    "    _, n_feat = pred.shape\n",
    "    v1 = pred[:,:int(n_feat/2)]\n",
    "    v2 = pred[:,int(n_feat/2):]\n",
    "    \n",
    "    # Compute the cosine similarity. Using `tf.math.reduce_sum`. \n",
    "    # Don't forget to use the appropriate axis argument.\n",
    "    dot = tf.math.reduce_sum(v1*v2, axis = 1)\n",
    "    norm = tf.math.sqrt(\n",
    "        tf.math.reduce_sum(v1 * v1, axis=1)\n",
    "        *\n",
    "        tf.math.reduce_sum(v2 * v2, axis=1))\n",
    "    d  = dot / norm\n",
    "    # Check if d>threshold to make predictions\n",
    "    y_pred = tf.cast(d>threshold, tf.float64)\n",
    "    # take the average of correct predictions to get the accuracy\n",
    "    accuracy = tf.math.reduce_mean(tf.cast(tf.equal(y_pred, y_test), tf.float64))\n",
    "    # compute the confusion matrix using `tf.math.confusion_matrix`\n",
    "    cm = tf.math.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dbab00d5-d7ce-453e-8a6d-5c0d552db611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step\n",
      "Accuracy 0.668359375\n",
      "Confusion matrix:\n",
      "[[4237 2145]\n",
      " [1251 2607]]\n"
     ]
    }
   ],
   "source": [
    "# this takes around 1 minute\n",
    "accuracy, cm = classify(Q1_test,Q2_test, y_test, 0.7, model,  batch_size = 512) \n",
    "print(\"Accuracy\", accuracy.numpy())\n",
    "print(f\"Confusion matrix:\\n{cm.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "25846374-4e5f-437a-968d-a8e90e5760d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(question1, question2, threshold, model, verbose=False):\n",
    "    \"\"\"Function for predicting if two questions are duplicates.\n",
    "\n",
    "    Args:\n",
    "        question1 (str): First question.\n",
    "        question2 (str): Second question.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (tensorflow.keras.Model): The Siamese model.\n",
    "        verbose (bool, optional): If the results should be printed out. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the questions are duplicates, False otherwise.\n",
    "    \"\"\"\n",
    "    generator = tf.data.Dataset.from_tensor_slices((([question1], [question2]),None)).batch(batch_size=1)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Call the predict method of your model and save the output into v1v2\n",
    "    v1v2 = model.predict(generator)\n",
    "    # Extract v1 and v2 from the model output\n",
    "    v1 = v1v2[:,:int(v1v2.shape[1]/2)]\n",
    "    v2 = v1v2[:,int(v1v2.shape[1]/2):]\n",
    "    # Take the dot product to compute cos similarity of each pair of entries, v1, v2\n",
    "    # Since v1 and v2 are both vectors, use the function tf.math.reduce_sum instead of tf.linalg.matmul\n",
    "    dot = tf.math.reduce_sum(v1*v2, axis = 1)\n",
    "    norm = tf.math.sqrt(\n",
    "        tf.math.reduce_sum(v1 * v1, axis=1)\n",
    "        *\n",
    "        tf.math.reduce_sum(v2 * v2, axis=1))\n",
    "    d = dot / norm\n",
    "    # Is d greater than the threshold?\n",
    "    res = d > threshold\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"Q1  = \", question1, \"\\nQ2  = \", question2)\n",
    "        print(\"d   = \", d.numpy())\n",
    "        print(\"res = \", res.numpy())\n",
    "\n",
    "    return res.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ff4c1492-d56c-411c-947a-545ef692dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
      "Q1  =  When will I see you? \n",
      "Q2  =  When can I see you again?\n",
      "d   =  [0.6716684]\n",
      "res =  [False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to try with your own questions\n",
    "question1 = \"When will I see you?\"\n",
    "question2 = \"When can I see you again?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(question1 , question2, 0.7, model, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5cdd8321-83d4-49cb-84e9-01e18ec92f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Q1  =  Do they enjoy eating the dessert? \n",
      "Q2  =  Do they like hiking in the desert?\n",
      "d   =  [0.01285205]\n",
      "res =  [False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to try with your own questions\n",
    "question1 = \"Do they enjoy eating the dessert?\"\n",
    "question2 = \"Do they like hiking in the desert?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(question1 , question2, 0.7, model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d92d3d-f4ce-405a-bcc3-8872a628fbb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
